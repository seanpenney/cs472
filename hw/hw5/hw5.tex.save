\documentclass[letterpaper,10pt,titlepage]{article}

\usepackage{graphicx}                                        
\usepackage{amssymb}                                         
\usepackage{amsmath}                                         
\usepackage{amsthm}                                          

\usepackage{alltt}                                           
\usepackage{float}
\usepackage{color}
\usepackage{url}

\usepackage{balance}
\usepackage[TABBOTCAP, tight]{subfigure}
\usepackage{enumitem}
\usepackage{pstricks, pst-node}

\usepackage{geometry}
\geometry{textheight=9in, textwidth=6.5in}

\newcommand{\cred}[1]{{\color{red}#1}}
\newcommand{\cblue}[1]{{\color{blue}#1}}

\usepackage{hyperref}
\usepackage{geometry}

\usepackage{listings}


\def\name{Sean Penney and Paul Atkinson}

%% The following metadata will show up in the PDF properties
\hypersetup{
  colorlinks = true,
  urlcolor = black,
  pdfauthor = {\name},
  pdfkeywords = {cs472 ``computer architecture'' clements ``chapter 9''},
  pdftitle = {CS 472: Homework 5},
  pdfsubject = {CS 472: Homework 5},
  pdfpagemode = UseNone
}

\begin{document}
\hfill \name

\hfill \today

\hfill CS 472 HW 5

\begin{enumerate}
\item[$(9.2)$] 

  The cache speeds up memory access by storing more frequently accessed data in the cache, rather than the slower main memory.

\item[$(9.3)$]

  Temporal locality refers to reusing specific data in a small amount of time, and spatial locality refers to reusing data that is grouped together.    

\item[$(9.4)$]

  $S = t_{m} / (h * t_{c} + (1 - h)t_{m})$, where $t_{m}$ is access time of main memory, $t_{c}$ is the access time of cache memory, and h is the hit ratio.
 
  This formula simplifies to:
 
  $S = 1 / (1 - h(1 - k))$, where k is the ratio of the access time to cache memory divided by the access time to main memory.
  
\item[$(9.5)$]
  
  a.  S = 526.3\%
 
  b.  S = 689.6\%
 
  c.  S = 416.67\%
 
  d.  S = 1273.89\%
  
\item[$(9.6)$]

  a.  h = .0957
 
  b.  h = .5263
 
  c.  h = .8421
 
  d.  h = .9825

\item[$(9.8)$]
  
  $t_{m} / t_{c}$ ??
  
\item[$(9.11)$]   
  
  a.  Word is a unit of data
  
  b.  A line is made up of individual words.
  
  c.  A set is a group of lines??
  
\item[$(9.12)$]
  
  a.  Using direct mapped cache, the data is stored like a simple table, with columns for data and tags.
 
  b.  A fully associative cache allows you to store the data anywhere in the cache.

  c.  A set-associative cache is a hybrid between a fully associative and direct mapped cache.  There is a general location in which the data is to be stored, and within that general location the data can be stored anywhere.
  
\item[$(9.17)$]

  Cache coherency refers to whether the data is consistent.  Write operations should occur instantaneously, processors should see the same sequence of changes of values, etc.
  
\item[$(9.22)$]

  Data caches are easier to implement than instruction caches since the contents of data caches are not modified.

\item[$(9.23)$]

  The average access time of a system with a cache that's accessed in parallel with main store is $t_{ave} = ht_{c} + (1-h)t_{m}$
  
  Let $t_{1}$ be the time to fetch a line from main store to reload the cache on a miss.  We must add $(1-h)t_{1}$ to the average access time.
  
  $t_{ave} = ht_{c} + (1-h)t_{m} + (1-h)t_{1}$

\item[$(9.26)$]

  The average is 50 + 1.1111 + 4.9985 = around 56.1 cycles

\item[$(9.28)$]

  The local miss rate is the number of misses in a specific cache divided by the total number of memory accesses to this cache.
  
  The global miss rate is the misses in a specific cache divided by the total number of memory accesses by the CPU.

\item[$(9.35)$]

  Since there are 4 sections, 8/4 = 2 MB each. 62500 lines, and since 2 to the 16 is 65536, the set is 16 bits long. There
  are 5 offset bits, as 2 to the 5th = 32 bytes (the line size).

\item[$(9.41)$]

  CPU cache is a portion of memory made of high-speed static RAM.
  
  Disk caching uses same principles as CPU caches, however, disk caching uses conventional main memory instead of SRAM.

\item[$(9.42)$]

  Write back means that a write operation to the main memory takes place only when a line in the cache is to be ejected, 
  while write through can just do the write operation anytime.

\item[$(9.43)$]

  The amount of space that the system may be able to address is $2^{32}$ = 4GB.
  
  The number of page table entries is 4 GB / 4 KB = 1 million.
  
  There are 4 bytes (32 bits) in a page table entry.
  
  1 million * 32 bits = 32 Mb or 4 MB

\item[$(9.45)$]

  The average cost of an instruction is 1.3 + . 5 + 5 = 6.8 cycles per instruction on average.

\item[$(9.46)$]

  To access the next element in the y[i] array, a read to main memory will be required.
  
  x and s will be cached, presumably in the L1 cache.
  
  Therefore, the access time is 50 cycles + 2 cycles + 2 cycles = 54 cycles for one iteration of the loop.

\item[$(9.57)$]
  
  a. 

  b. 

  c. 32 byte line size, set is 

\begin{lstlisting}
\end{lstlisting}
  
\end{enumerate}



\end{document}
